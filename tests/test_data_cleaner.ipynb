{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b07f05b-7cdc-4513-9116-70ed75e0aaea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/ovidiumtoma@gmail.com/wind_turbine_project/src/wt_cleaning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b07ccf3-e7f3-4098-a8a6-abe82ba9c1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"TestDataCleaner\") \\\n",
    "        .getOrCreate()\n",
    "    yield spark\n",
    "    spark.stop()\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def cleaner(spark):\n",
    "    return DataCleaner(spark)\n",
    "\n",
    "def test_clean_turbine_data(spark, cleaner):\n",
    "    # Create dummy data with some rows needing removal.\n",
    "    data = [\n",
    "        (\"2021-01-01 00:00:00\", 10.0, 5.0, 1),\n",
    "        (\"2021-01-01 00:01:00\", -5.0, 3.0, 1),   # negative power_output becomes null and then dropped\n",
    "        (\"2021-01-01 00:02:00\", 15.0, None, 1),    # missing wind_speed, drop row\n",
    "        (\"2021-01-01 00:03:00\", 20.0, 7.0, 1)        # valid row\n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df_clean = cleaner.clean_turbine_data(df)\n",
    "    # Expect only rows with valid essential values (rows 1 and 4)\n",
    "    assert df_clean.count() == 2\n",
    "\n",
    "def test_transform_turbine_data(spark, cleaner):\n",
    "    # Create dummy new turbine data.\n",
    "    data = [\n",
    "        (\"2021-01-01 12:00:00\", 0.5, 10.0, \"45\")\n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"Time\", StringType(), True),\n",
    "        StructField(\"Power\", DoubleType(), True),\n",
    "        StructField(\"windspeed_100m\", DoubleType(), True),\n",
    "        StructField(\"winddirection_100m\", StringType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df_transformed = cleaner.transform_turbine_data(df)\n",
    "    cols = df_transformed.columns\n",
    "    assert \"power_output\" in cols\n",
    "    assert \"timestamp\" in cols\n",
    "    assert \"wind_speed\" in cols\n",
    "    assert \"wind_direction\" in cols\n",
    "    assert \"Time\" not in cols\n",
    "    assert \"Power\" not in cols\n",
    "    # Check that power scaling is correct: 0.5 * 4.5 = 2.25\n",
    "    result = df_transformed.select(\"power_output\").collect()[0][\"power_output\"]\n",
    "    assert result == pytest.approx(2.25, rel=1e-2)\n",
    "\n",
    "def test_merge_bronze_data(spark, cleaner):\n",
    "    # Create dummy original and new DataFrames.\n",
    "    original_data = [(\"2021-01-01 00:00:00\", 10.0, 5.0, 1)]\n",
    "    new_data = [(\"2021-01-01 00:01:00\", 20.0, 6.0, 2)]\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True)\n",
    "    ])\n",
    "    df_orig = spark.createDataFrame(original_data, schema)\n",
    "    df_new = spark.createDataFrame(new_data, schema)\n",
    "    merged = cleaner.merge_bronze_data(df_orig, df_new)\n",
    "    assert merged.count() == 2\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_data_cleaner",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
