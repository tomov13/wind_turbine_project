{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e036cca8-20cf-4e9d-a85b-767a23290188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/ovidiumtoma@gmail.com/wind_turbine_project/src/wt_transformations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74b76b16-649c-460a-bf32-1da40709b8bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from data_transformer import DataTransformer\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"TestDataTransformer\") \\\n",
    "        .getOrCreate()\n",
    "    yield spark\n",
    "    spark.stop()\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def transformer(spark):\n",
    "    return DataTransformer(spark)\n",
    "\n",
    "def test_compute_expected_power(spark, transformer):\n",
    "    data = [(\"2021-01-01 00:00:00\", 10.0, 5.0, 1)]\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df = transformer.compute_expected_power(df)\n",
    "    expected_power = 0.5 * 1.2 * 5024 * (5 ** 3) * 0.45 / 1e6\n",
    "    result = df.select(\"expected_power\").collect()[0][\"expected_power\"]\n",
    "    assert result == pytest.approx(expected_power, rel=1e-4)\n",
    "\n",
    "def test_detect_zscore_anomalies(spark, transformer):\n",
    "    data = [\n",
    "        (\"2021-01-01 00:00:00\", 10.0, 5.0, 1),\n",
    "        (\"2021-01-01 00:01:00\", 15.0, 5.0, 1),\n",
    "        (\"2021-01-01 00:02:00\", 20.0, 5.0, 1)\n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df = transformer.compute_expected_power(df)\n",
    "    df = transformer.detect_zscore_anomalies(df)\n",
    "    assert \"z_anomaly\" in df.columns\n",
    "    anomalies = [row[\"z_anomaly\"] for row in df.select(\"z_anomaly\").collect()]\n",
    "    for a in anomalies:\n",
    "        assert a in [0, 1]\n",
    "\n",
    "def test_detect_record_anomalies(spark, transformer):\n",
    "    data = [\n",
    "        (\"2021-01-01 00:00:00\", 10.0, 5.0, 1),\n",
    "        (\"2021-01-01 00:01:00\", 15.0, 5.0, 1),\n",
    "        (\"2021-01-01 00:02:00\", 20.0, 5.0, 1)\n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df = transformer.compute_expected_power(df)\n",
    "    df = transformer.detect_record_anomalies(df)\n",
    "    assert \"if_anomaly\" in df.columns\n",
    "    anomalies = [row[\"if_anomaly\"] for row in df.select(\"if_anomaly\").collect()]\n",
    "    for a in anomalies:\n",
    "        assert a in [0, 1]\n",
    "\n",
    "def test_combine_anomalies(spark, transformer):\n",
    "    data = [\n",
    "        (1, 0, 0),\n",
    "        (1, 1, 0),\n",
    "        (1, 0, 1),\n",
    "        (1, 1, 1)\n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"turbine_id\", IntegerType(), True),\n",
    "        StructField(\"z_anomaly\", IntegerType(), True),\n",
    "        StructField(\"if_anomaly\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df = transformer.combine_anomalies(df)\n",
    "    combined = [row[\"combined_anomaly\"] for row in df.select(\"combined_anomaly\").collect()]\n",
    "    assert combined == [0, 1, 1, 1]\n",
    "\n",
    "def test_detect_turbine_anomalies(spark, transformer):\n",
    "    data = [\n",
    "        (\"2021-01-01 00:00:00\", 10.0, 5.0, 1, 1),\n",
    "        (\"2021-01-01 00:01:00\", 10.0, 5.0, 1, 1),\n",
    "        (\"2021-01-01 00:00:00\", 20.0, 6.0, 2, 0),\n",
    "        (\"2021-01-01 00:01:00\", 20.0, 6.0, 2, 0)\n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True),\n",
    "        StructField(\"combined_anomaly\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    turbine_anomalies = transformer.detect_turbine_anomalies(df)\n",
    "    results = {row[\"turbine_id\"]: row[\"turbine_status\"] for row in turbine_anomalies.collect()}\n",
    "    assert results[1] == \"FAULTY_SENSOR\"\n",
    "    assert results[2] == \"NORMAL\"\n",
    "\n",
    "def test_apply_smart_filtering(spark, transformer):\n",
    "    data = [\n",
    "        (1, \"FAULTY_SENSOR\", 10.0),\n",
    "        (1, \"NORMAL\", 15.0),\n",
    "        (2, \"REVIEW_REQUIRED\", 20.0),\n",
    "        (2, \"NORMAL\", 25.0)\n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"turbine_id\", IntegerType(), True),\n",
    "        StructField(\"turbine_status\", StringType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    filtered = transformer.apply_smart_filtering(df, df)\n",
    "    statuses = [row[\"turbine_status\"] for row in filtered.select(\"turbine_status\").collect()]\n",
    "    assert \"FAULTY_SENSOR\" not in statuses\n",
    "\n",
    "def test_calculate_summary_statistics(spark, transformer):\n",
    "    data = [\n",
    "        (\"2021-01-01 00:00:00\", 10.0, 1),\n",
    "        (\"2021-01-01 00:30:00\", 20.0, 1),\n",
    "        (\"2021-01-01 01:00:00\", 30.0, 1)\n",
    "    ]\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    summary = transformer.calculate_summary_statistics(df)\n",
    "    # Expect one summary row for turbine_id 1.\n",
    "    assert summary.count() == 1\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_data_transformer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
