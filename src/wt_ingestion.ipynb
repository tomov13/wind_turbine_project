{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6106d8-977d-4cbf-8c1c-be48402b5cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/ovidiumtoma@gmail.com/wind_turbine_project/src/wt_logger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac83142f-ee36-4f06-b102-6927b9990ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import regexp_extract, lit, col\n",
    "\n",
    "class RawDataIngestor:\n",
    "    def __init__(self, spark, config=None):\n",
    "        self.spark = spark\n",
    "        self.logger = LoggerUtility.setup_logging()\n",
    "        self.config = config or {}\n",
    "        self.logger.info(\"RawDataIngestor initialized.\")\n",
    "\n",
    "    def load_original_turbine_data(self, directory_path: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the original dataset from CSV with schema inference.\n",
    "        Assumes the CSV already has a 'turbine_id' column.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Loading original turbine data from {directory_path} (with inferSchema)\")\n",
    "            df = (self.spark.read\n",
    "                  .format(\"csv\")\n",
    "                  .option(\"header\", \"true\")\n",
    "                  .option(\"inferSchema\", \"true\")\n",
    "                  .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Enable file metadata\n",
    "                  .load(f\"{directory_path}/data_group_*.csv\")\n",
    "                  .cache())\n",
    "\n",
    "            record_count = df.count()\n",
    "            self.logger.info(f\"Successfully loaded {record_count} original turbine dataset records.\")\n",
    "            self.logger.info(\"Using 'turbine_id' from the CSV itself.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading original turbine data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_new_turbine_data(self, directory_path: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Loads new turbine data from CSV with schema inference.\n",
    "        Uses max_existing_id to assign a unique turbine_id.\n",
    "        Assumes filenames follow a pattern like 'Location<number>.csv' (case-insensitive).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Loading new turbine data from {directory_path} (with inferSchema)\")\n",
    "            df = (self.spark.read\n",
    "                  .format(\"csv\")\n",
    "                  .option(\"header\", \"true\")\n",
    "                  .option(\"inferSchema\", \"true\")\n",
    "                  .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Ensure metadata is captured\n",
    "                  .load(f\"{directory_path}/Location*.csv\")\n",
    "                  .withColumn(\"_metadata.file_path\", col(\"_metadata.file_path\"))  # Capture file path\n",
    "                  .cache())\n",
    "\n",
    "            record_count = df.count()\n",
    "            self.logger.info(f\"Successfully loaded {record_count} new turbine dataset records.\")\n",
    "\n",
    "            # Get the maximum turbine_id from the original dataset.\n",
    "            df_original_turbine_data = self.spark.read.table(\"bronze_data.original_turbine_bronze\")\n",
    "            max_existing_id = df_original_turbine_data.agg({\"turbine_id\": \"max\"}).collect()[0][0]\n",
    "            if max_existing_id is None:\n",
    "                max_existing_id = 0\n",
    "\n",
    "            self.logger.info(f\"Using maximum existing turbine_id: {max_existing_id}\")\n",
    "\n",
    "            # Extract turbine number from file metadata column\n",
    "            df = df.withColumn(\n",
    "                \"extracted_turbine_id\",\n",
    "                regexp_extract(col(\"_metadata.file_path\"), \"Location(\\\\d+)\", 1)\n",
    "            )\n",
    "\n",
    "            # Convert to integer, add the offset, and overwrite turbine_id column.\n",
    "            df = df.withColumn(\n",
    "                \"turbine_id\",\n",
    "                col(\"extracted_turbine_id\").cast(\"int\") + lit(max_existing_id)\n",
    "            ).drop(\"extracted_turbine_id\")\n",
    "\n",
    "            self.logger.info(\"Successfully added turbine_id to new turbine dataset records.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading new turbine data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def write_bronze(self, df: DataFrame, table_name: str):\n",
    "        \"\"\"Writes the given DataFrame to a Bronze Delta table.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Writing data to Delta table: bronze_data.{table_name}\")\n",
    "            self.spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze_data\")\n",
    "            df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"bronze_data.{table_name}\")\n",
    "            self.logger.info(f\"Successfully written data to bronze_data.{table_name}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error writing bronze data: {e}\")\n",
    "            raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wt_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
