{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6106d8-977d-4cbf-8c1c-be48402b5cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/ovidiumtoma@gmail.com/wind_turbine_project/src/wt_logger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac83142f-ee36-4f06-b102-6927b9990ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, regexp_extract, lit\n",
    "\n",
    "class RawDataIngestor:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.logger = LoggerUtility.setup_logging()\n",
    "        self.logger.info(\"RawDataIngestor initialized.\")\n",
    "\n",
    "    def load_original_turbine_data(self, directory_path):\n",
    "        \"\"\" Loads raw original dataset files into a Bronze table. \"\"\"\n",
    "\n",
    "        self.logger.info(f\"Loading original turbine data from {directory_path}\")\n",
    "\n",
    "        df = (self.spark.read.format(\"csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"inferSchema\", \"true\")\n",
    "              .load(f\"{directory_path}/data_group_*.csv\"))\n",
    "\n",
    "        self.logger.info(f\"Successfully loaded {df.count()} original turbine dataset records.\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def load_new_turbine_data(self, directory_path):\n",
    "        \"\"\" Loads raw new dataset files into a Bronze table. \"\"\"\n",
    "\n",
    "        self.logger.info(f\"Loading new turbine data from {directory_path}\")\n",
    "\n",
    "        df = (self.spark.read.format(\"csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"inferSchema\", \"true\")\n",
    "              .load(f\"{directory_path}/Location*.csv\"))\n",
    "\n",
    "        self.logger.info(f\"Successfully loaded {df.count()} new turbine dataset records.\")\n",
    "\n",
    "        # Get the maximum turbine_id from the original dataset\n",
    "        max_existing_id = df_original_turbine_data.agg({\"turbine_id\": \"max\"}).collect()[0][0]\n",
    "        if max_existing_id is None:\n",
    "            max_existing_id = 0  # In case no existing IDs\n",
    "\n",
    "        self.logger.info(f\"Maximum ID number from original dataset: {max_existing_id}\")\n",
    "        \n",
    "        # Assign turbine ID based on file name \n",
    "\n",
    "        df = df.withColumn(\"turbine_id\",\n",
    "                           (regexp_extract(input_file_name(), \"Location(\\d+)\", 1).cast(\"int\") + lit(max_existing_id)))\n",
    "\n",
    "        self.logger.info(f\"Successfully added turbine_id to new turbine dataset records.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def write_bronze(self, df, table_name):\n",
    "        \"\"\" Writes the given DataFrame to a Bronze Delta Table. \"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Writing data to Delta table: bronze_data.{table_name}\")\n",
    "        self.spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze_data\")\n",
    "        df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"bronze_data.{table_name}\")\n",
    "        self.logger.info(f\"Successfully written data to bronze_data.{table_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wt_ingestion",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
