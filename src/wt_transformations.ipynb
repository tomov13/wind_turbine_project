{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "411e6ec4-9fef-4b05-a87d-fc5e35d757dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import mean, stddev, min as spark_min, max as spark_max\n",
    "\n",
    "# class DataTransformer:\n",
    "#     def __init__(self, spark):\n",
    "#         self.spark = spark\n",
    "\n",
    "#     def summarize_power_output(self, df):\n",
    "#         return (df.groupBy(\"turbine_id\")\n",
    "#                   .agg(\n",
    "#                       spark_min(\"power_output\").alias(\"min_power\"),\n",
    "#                       spark_max(\"power_output\").alias(\"max_power\"),\n",
    "#                       mean(\"power_output\").alias(\"avg_power\"),\n",
    "#                       stddev(\"power_output\").alias(\"stddev_power\")\n",
    "#                   ))\n",
    "\n",
    "#     def identify_anomalies(self, summary_df):\n",
    "#         # We assume the \"expected\" power output = the average, \n",
    "#         # and anomalies are >2 std dev away from mean\n",
    "#         from pyspark.sql.functions import col\n",
    "#         # We might join the summary stats back to the original or handle it directly here\n",
    "#         # For demonstration, let's just produce a DF of anomalies from the summary.\n",
    "#         return (summary_df\n",
    "#                 .withColumn(\"is_anomaly\", \n",
    "#                     (col(\"max_power\") - col(\"avg_power\") > 2*col(\"stddev_power\")) |\n",
    "#                     (col(\"min_power\") - col(\"avg_power\") < -2*col(\"stddev_power\"))\n",
    "#                 )\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28c177d6-aad3-44b0-b8e7-8fc6c651cd67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/ovidiumtoma@gmail.com/wind_turbine_project/src/wt_logger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b785ba-8d47-4996-85ed-d06720e331e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import mean, stddev, min as spark_min, max as spark_max, col\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self, spark):\n",
    "        \"\"\" Initializes the DataTransformer class with a Spark session and logger. \"\"\"\n",
    "        self.spark = spark\n",
    "        self.logger = LoggerUtility.setup_logging()\n",
    "\n",
    "        self.logger.info(\"DataTransformer initialized.\")\n",
    "\n",
    "    def summarize_power_output(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\" \n",
    "        Calculates summary statistics for power output per turbine. \n",
    "        Produces min, max, average, and standard deviation of power output.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Summarizing power output...\")\n",
    "\n",
    "        summary_df = (df.groupBy(\"turbine_id\")\n",
    "                        .agg(\n",
    "                            spark_min(\"power_output\").alias(\"min_power\"),\n",
    "                            spark_max(\"power_output\").alias(\"max_power\"),\n",
    "                            mean(\"power_output\").alias(\"avg_power\"),\n",
    "                            stddev(\"power_output\").alias(\"stddev_power\")\n",
    "                        ))\n",
    "\n",
    "        self.logger.info(f\"Summary statistics calculated. Row count: {summary_df.count()}\")\n",
    "        return summary_df\n",
    "\n",
    "    def identify_anomalies(self, summary_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Identifies anomalies where power output deviates more than 2 standard deviations from the mean.\n",
    "        Returns a DataFrame with an \"is_anomaly\" column indicating anomalies.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Identifying anomalies...\")\n",
    "\n",
    "        anomalies_df = (summary_df\n",
    "                        .withColumn(\"is_anomaly\", \n",
    "                                    (col(\"max_power\") - col(\"avg_power\") > 2 * col(\"stddev_power\")) |\n",
    "                                    (col(\"min_power\") - col(\"avg_power\") < -2 * col(\"stddev_power\"))\n",
    "                        ))\n",
    "\n",
    "        anomaly_count = anomalies_df.filter(col(\"is_anomaly\") == True).count()\n",
    "        self.logger.info(f\"Anomaly detection complete. Anomalies found: {anomaly_count}\")\n",
    "        return anomalies_df\n",
    "\n",
    "    def save_gold_table(self, df: DataFrame, table_name: str):\n",
    "        \"\"\"\n",
    "        Saves the transformed (summary/anomaly) data as a Delta table in the `gold_data` schema.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Saving transformed data to gold_data.{table_name}...\")\n",
    "\n",
    "        try:\n",
    "            # Ensure the schema exists\n",
    "            self.spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold_data\")\n",
    "\n",
    "            # Save the DataFrame as a Delta table\n",
    "            df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"gold_data.{table_name}\")\n",
    "\n",
    "            self.logger.info(f\"Successfully saved transformed data to gold_data.{table_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving transformed data to gold_data.{table_name}: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def transform_and_save(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Runs the full transformation pipeline:\n",
    "        1. Summarizes power output.\n",
    "        2. Identifies anomalies.\n",
    "        3. Saves results to Gold tables.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting transformation process...\")\n",
    "\n",
    "        summary_df = self.summarize_power_output(df)\n",
    "        anomalies_df = self.identify_anomalies(summary_df)\n",
    "\n",
    "        # Save both summary and anomalies\n",
    "        self.save_gold_table(summary_df, \"turbine_summary\")\n",
    "        self.save_gold_table(anomalies_df, \"turbine_anomalies\")\n",
    "\n",
    "        self.logger.info(\"Transformation process complete.\")\n",
    "        return summary_df, anomalies_df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wt_transformations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
