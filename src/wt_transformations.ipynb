{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c177d6-aad3-44b0-b8e7-8fc6c651cd67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/ovidiumtoma@gmail.com/wind_turbine_project/src/wt_logger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdb81319-8158-4f53-8e03-02e0d28a7cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (mean, stddev, col, when, abs as spark_abs,\n",
    "                                  lit, pow, monotonically_increasing_id, count, sum as spark_sum,\n",
    "                                  min as spark_min, max as spark_max, avg as spark_avg, round, window)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self, spark):\n",
    "        \"\"\" Initializes the DataTransformer class with a Spark session and logger. \"\"\"\n",
    "        self.spark = spark\n",
    "        self.logger = LoggerUtility.setup_logging()\n",
    "        self.logger.info(\"DataTransformer initialized.\")\n",
    "\n",
    "    def compute_expected_power(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\" \n",
    "        Compute expected power output using a simplified wind power equation.\n",
    "        \n",
    "        Formula: P = 0.5 * ρ * A * V³ * Cp\n",
    "        Where:\n",
    "        - ρ = 1.2 kg/m³ (air density)\n",
    "        - A = 5024 m² (swept area for ~100m rotor)\n",
    "        - Cp = 0.45 (power coefficient)\n",
    "        - V = wind speed (m/s)\n",
    "        \n",
    "        Converts power from watts to MW (dividing by 1,000,000).\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Computing expected power output...\")\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"expected_power\",\n",
    "            0.5 * lit(1.2) *\n",
    "            lit(5024) *\n",
    "            pow(col(\"wind_speed\"), 3) *\n",
    "            lit(0.45) / lit(1_000_000)\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"Expected power computation complete.\")\n",
    "        return df\n",
    "\n",
    "    def detect_zscore_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Detect anomalies using a simple ±2 standard deviation check on the error:\n",
    "            error = (power_output - expected_power)\n",
    "\n",
    "        - Compute mean and stddev of error\n",
    "        - Flag records as \"z_anomaly\" = 1 if error is outside mean ± 2 * stddev,\n",
    "          else 0\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Detecting anomalies using ±2σ on the error (z-score method)...\")\n",
    "\n",
    "        # 1) Create error column\n",
    "        df = df.withColumn(\"error\", col(\"power_output\") - col(\"expected_power\"))\n",
    "\n",
    "        # 2) Collect mean and stddev\n",
    "        stats = (df\n",
    "                 .select(\n",
    "                     mean(col(\"error\")).alias(\"mean_error\"),\n",
    "                     stddev(col(\"error\")).alias(\"std_error\")\n",
    "                 )\n",
    "                 .collect()[0])\n",
    "        mean_err = stats[\"mean_error\"]\n",
    "        std_err  = stats[\"std_error\"]\n",
    "\n",
    "        # 3) Flag anomalies outside mean ± 2*std\n",
    "        threshold = 2.0\n",
    "        df = df.withColumn(\n",
    "            \"z_anomaly\",\n",
    "            when(\n",
    "                spark_abs(col(\"error\") - lit(mean_err)) > threshold * lit(std_err),\n",
    "                lit(1)\n",
    "            ).otherwise(lit(0))\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"Z-score anomaly detection complete.\")\n",
    "        return df\n",
    "\n",
    "    def detect_record_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Detects anomalies using Isolation Forest (multivariate).\n",
    "        \n",
    "        - Uses wind_speed, expected_power, and power_output as features.\n",
    "        - Contamination rate is 15% by default (change if needed).\n",
    "        - Returns a column \"iso_anomaly\" = 1 if anomaly, else 0.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Detecting record-level anomalies with Isolation Forest...\")\n",
    "\n",
    "        # Add unique record ID for join-back\n",
    "        df = df.withColumn(\"record_id\", monotonically_increasing_id())\n",
    "\n",
    "        # Convert relevant columns to Pandas\n",
    "        pdf = (df\n",
    "               .select(\"record_id\", \"wind_speed\", \"expected_power\", \"power_output\")\n",
    "               .toPandas()\n",
    "               )\n",
    "\n",
    "        # Fit Isolation Forest\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=0.15,  # adjust as needed\n",
    "            n_estimators=200,\n",
    "            max_samples='auto',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Predict anomalies\n",
    "        pdf[\"iso_score\"] = iso_forest.fit_predict(\n",
    "            pdf[[\"wind_speed\", \"expected_power\", \"power_output\"]]\n",
    "        )\n",
    "\n",
    "        # Convert -1 to 1 (anomaly), 1 to 0 (normal)\n",
    "        pdf[\"iso_score\"] = pdf[\"iso_score\"].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "        # Prepare a Pandas subset for joining\n",
    "        anomaly_pdf = pdf[[\"record_id\", \"iso_score\"]]\n",
    "\n",
    "        # Convert back to Spark\n",
    "        anomaly_sdf = self.spark.createDataFrame(anomaly_pdf)\n",
    "\n",
    "        # Join on record_id\n",
    "        df = df.join(anomaly_sdf, on=\"record_id\", how=\"left\")\n",
    "\n",
    "        # Rename to iso_anomaly\n",
    "        df = df.withColumnRenamed(\"iso_score\", \"iso_anomaly\")\n",
    "\n",
    "        self.logger.info(\"Isolation Forest anomaly detection complete.\")\n",
    "        return df\n",
    "\n",
    "    def combine_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create a 'combined_anomaly' column using the OR of z_anomaly and iso_anomaly.\n",
    "        Keeps all anomaly columns in the final DataFrame.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Combining anomalies with OR logic (z_anomaly OR iso_anomaly).\")\n",
    "        df = df.withColumn(\n",
    "            \"combined_anomaly\",\n",
    "            when((col(\"z_anomaly\") == 1) | (col(\"iso_anomaly\") == 1), lit(1)).otherwise(lit(0))\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def detect_turbine_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Detects turbines with an unusually high number of anomalies.\n",
    "        \n",
    "        - Groups by turbine_id and calculates the anomaly rate (based on combined_anomaly).\n",
    "        - Classification:\n",
    "            > 60% → FAULTY_SENSOR\n",
    "            30-60% → REVIEW_REQUIRED\n",
    "            < 30% → NORMAL\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Detecting turbines with high anomaly rates...\")\n",
    "\n",
    "        turbine_anomaly_df = (\n",
    "            df.groupBy(\"turbine_id\")\n",
    "            .agg(\n",
    "                spark_sum(\"combined_anomaly\").alias(\"total_anomalies\"),\n",
    "                count(\"*\").alias(\"total_records\")\n",
    "            )\n",
    "            .withColumn(\"anomaly_rate\", col(\"total_anomalies\") / col(\"total_records\"))\n",
    "        )\n",
    "\n",
    "        turbine_anomaly_df = turbine_anomaly_df.withColumn(\n",
    "            \"turbine_status\",\n",
    "            when(col(\"anomaly_rate\") > 0.6, \"FAULTY_SENSOR\")\n",
    "            .when(col(\"anomaly_rate\") > 0.3, \"REVIEW_REQUIRED\")\n",
    "            .otherwise(\"NORMAL\")\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"Turbine anomaly detection complete.\")\n",
    "        return turbine_anomaly_df\n",
    "\n",
    "    def apply_smart_filtering(self, df: DataFrame, turbine_anomaly_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Applies filtering based on turbine anomaly classification:\n",
    "        \n",
    "        - FAULTY_SENSOR turbines are removed entirely.\n",
    "        - REVIEW_REQUIRED turbines remain but can be flagged.\n",
    "        - NORMAL turbines remain.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Applying smart filtering...\")\n",
    "\n",
    "        # Join status onto each record\n",
    "        df = df.join(turbine_anomaly_df.select(\"turbine_id\", \"turbine_status\"),\n",
    "                     on=\"turbine_id\", how=\"left\")\n",
    "\n",
    "        # Filter out FAULTY_SENSOR turbines\n",
    "        df = df.filter(col(\"turbine_status\") != \"FAULTY_SENSOR\")\n",
    "\n",
    "        self.logger.info(\"Smart filtering complete.\")\n",
    "        return df\n",
    "    \n",
    "    def calculate_summary_statistics(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Computes summary statistics (min, max, avg power output) per turbine over a 24-hour window.\n",
    "\n",
    "        Returns a DataFrame with:\n",
    "        - turbine_id\n",
    "        - 24-hour window start\n",
    "        - min power output\n",
    "        - max power output\n",
    "        - avg power output\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(\"Calculating summary statistics over a 24-hour period...\")\n",
    "\n",
    "        summary_df = (\n",
    "            df.groupBy(\"turbine_id\", window(col(\"timestamp\"), \"24 hours\"))\n",
    "            .agg(\n",
    "                round(spark_min(\"power_output\"), 2).alias(\"min_power\"),\n",
    "                round(spark_max(\"power_output\"), 2).alias(\"max_power\"),\n",
    "                round(spark_avg(\"power_output\"), 2).alias(\"avg_power\")\n",
    "            )\n",
    "            .select(\n",
    "                col(\"turbine_id\"),\n",
    "                col(\"window.start\").alias(\"window_start\"),\n",
    "                col(\"min_power\"),\n",
    "                col(\"max_power\"),\n",
    "                col(\"avg_power\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"Summary statistics calculation complete.\")\n",
    "        return summary_df\n",
    "\n",
    "    def save_turbine_analysis(self, df: DataFrame, table_name: str = \"gold_turbine_analysis\"):\n",
    "        \"\"\"\n",
    "        Saves the final turbine dataset to the Gold layer.\n",
    "        \n",
    "        - Ensures the gold_data schema exists.\n",
    "        - Saves the dataset as a Delta table.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Saving results to gold_data.{table_name}\")\n",
    "        self.spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold_data\")\n",
    "        df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"gold_data.{table_name}\")\n",
    "        self.logger.info(f\"Successfully saved to gold_data.{table_name}\")\n",
    "\n",
    "    def save_summary_table(self, df: DataFrame, table_name: str = \"gold_turbine_summary\"):\n",
    "        \"\"\"\n",
    "        Saves summary statistics to the Gold layer.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Saving summary statistics to gold_data.{table_name}\")\n",
    "        self.spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold_data\")\n",
    "        df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"gold_data.{table_name}\")\n",
    "        self.logger.info(f\"Successfully saved summary statistics to gold_data.{table_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wt_transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
