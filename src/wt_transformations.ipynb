{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c177d6-aad3-44b0-b8e7-8fc6c651cd67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/ovidiumtoma@gmail.com/wind_turbine_project/src/wt_logger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdb81319-8158-4f53-8e03-02e0d28a7cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    def __init__(self, spark):\n",
    "        \"\"\" Initializes the DataTransformer class with a Spark session and logger. \"\"\"\n",
    "        self.spark = spark\n",
    "        self.logger = LoggerUtility.setup_logging()\n",
    "        self.logger.info(\"DataTransformer initialized.\")\n",
    "\n",
    "    def compute_expected_power(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\" \n",
    "        Compute expected power output using the wind power equation.\n",
    "\n",
    "        Formula: P = 0.5 * ρ * A * V³ * Cp\n",
    "        Where:\n",
    "        - ρ = 1.2 kg/m³ (air density)\n",
    "        - A = 5024 m² (swept area for a 100m turbine)\n",
    "        - Cp = 0.45 (power coefficient)\n",
    "        - V = wind speed (from dataset)\n",
    "        \n",
    "        Converts power from watts to MW (dividing by 1,000,000).\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(\"Computing expected power output...\")\n",
    "\n",
    "        df = df.withColumn(\"expected_power\",\n",
    "                           0.5 * lit(1.2) * lit(5024) *\n",
    "                           pow(col(\"wind_speed\"), 3) * lit(0.45) / 1_000_000\n",
    "                          )\n",
    "\n",
    "        self.logger.info(\"Expected power computation complete.\")\n",
    "        return df\n",
    "\n",
    "    def detect_record_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Detects anomalies at the record level using Isolation Forest.\n",
    "\n",
    "        - Takes wind speed, expected power, and actual power as input.\n",
    "        - Converts to Pandas for processing.\n",
    "        - Uses Isolation Forest to detect anomalies (contamination=5%).\n",
    "        - Joins results back into PySpark DataFrame.\n",
    "\n",
    "        If is_anomaly = 1, the record is considered an anomaly.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(\"Detecting anomalies using Isolation Forest...\")\n",
    "\n",
    "        # Adding unique ID per record\n",
    "        df = df.withColumn(\"record_id\", monotonically_increasing_id())\n",
    "\n",
    "        # Convert to Pandas (Isolation Forest requires NumPy)\n",
    "        pdf = df.select(\"record_id\", \"wind_speed\", \"expected_power\", \"power_output\").toPandas()\n",
    "\n",
    "        iso_forest = IsolationForest(\n",
    "                    contamination=0.15,  \n",
    "                    n_estimators=200,      \n",
    "                    max_samples='auto',\n",
    "                    random_state=42\n",
    "                )\n",
    "\n",
    "        pdf[\"anomaly_score\"] = iso_forest.fit_predict(\n",
    "            pdf[[\"wind_speed\", \"expected_power\", \"power_output\"]]\n",
    "        )\n",
    "\n",
    "        # Convert -1 (anomaly) to 1, and 1 (normal) to 0\n",
    "        pdf[\"anomaly_score\"] = pdf[\"anomaly_score\"].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "        # Create a Pandas DF with just the ID and the anomaly result\n",
    "        anomaly_pdf = pdf[[\"record_id\", \"anomaly_score\"]]\n",
    "\n",
    "        # Convert that to a Spark DataFrame\n",
    "        spark_pdf = spark.createDataFrame(anomaly_pdf)\n",
    "\n",
    "        # Join on record_id, not on power_output\n",
    "        df = df.join(spark_pdf, on=\"record_id\", how=\"left\").withColumnRenamed(\"anomaly_score\", \"is_anomaly\")\n",
    "\n",
    "\n",
    "        self.logger.info(\"Anomaly detection complete.\")\n",
    "        return df\n",
    "\n",
    "    def detect_turbine_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Detects turbines with an unusually high number of anomalies.\n",
    "\n",
    "        - Groups by turbine_id and calculates the anomaly rate.\n",
    "        - Classifies turbines based on their anomaly rates:\n",
    "          > 60% anomalies → FAULTY_SENSOR (Exclude)\n",
    "          30-60% anomalies → REVIEW_REQUIRED (Flag for review)\n",
    "          < 30% anomalies → NORMAL (Keep)\n",
    "\n",
    "        Returns a DataFrame with turbine_id and its anomaly classification.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(\"Detecting turbines with high anomaly rates...\")\n",
    "\n",
    "        turbine_anomaly_df = (df.groupBy(\"turbine_id\")\n",
    "                               .agg(spark_sum(\"is_anomaly\").alias(\"total_anomalies\"),\n",
    "                                    count(\"*\").alias(\"total_records\"))\n",
    "                               .withColumn(\"anomaly_rate\", col(\"total_anomalies\") / col(\"total_records\"))\n",
    "                              )\n",
    "\n",
    "        turbine_anomaly_df = turbine_anomaly_df.withColumn(\n",
    "            \"turbine_status\",\n",
    "            when(col(\"anomaly_rate\") > 0.6, \"FAULTY_SENSOR\")\n",
    "            .when(col(\"anomaly_rate\") > 0.3, \"REVIEW_REQUIRED\")\n",
    "            .otherwise(\"NORMAL\")\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"Turbine anomaly detection complete.\")\n",
    "        return turbine_anomaly_df\n",
    "\n",
    "    def apply_smart_filtering(self, df: DataFrame, turbine_anomaly_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Applies filtering rules based on turbine anomaly classification:\n",
    "\n",
    "        - FAULTY_SENSOR turbines are removed.\n",
    "        - REVIEW_REQUIRED turbines are flagged for review.\n",
    "        - NORMAL turbines remain unchanged.\n",
    "\n",
    "        Ensures the final dataset is clean and reliable.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(\"Applying smart filtering...\")\n",
    "\n",
    "        df = df.join(turbine_anomaly_df.select(\"turbine_id\", \"turbine_status\"), on=\"turbine_id\", how=\"left\")\n",
    "\n",
    "        df = df.filter(col(\"turbine_status\") != \"FAULTY_SENSOR\")\n",
    "\n",
    "        self.logger.info(\"Smart filtering complete.\")\n",
    "        return df\n",
    "\n",
    "    def save_gold_table(self, df: DataFrame, table_name: str = \"gold_turbine_analysis\"):\n",
    "        \"\"\"\n",
    "        Saves the final turbine dataset to the Gold layer.\n",
    "        \n",
    "        - Ensures the gold_data schema exists.\n",
    "        - Saves the dataset as a Delta Table.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(f\"Saving results to gold_data.{table_name}\")\n",
    "        self.spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold_data\")\n",
    "        df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"gold_data.{table_name}\")\n",
    "        self.logger.info(f\"Successfully saved to gold_data.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wt_transformations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
