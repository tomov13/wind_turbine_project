{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c177d6-aad3-44b0-b8e7-8fc6c651cd67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/ovidiumtoma@gmail.com/wind_turbine_project/src/wt_logger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdb81319-8158-4f53-8e03-02e0d28a7cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (col, lit, pow, round, mean, stddev, abs as spark_abs,\n",
    "                                   monotonically_increasing_id, count, sum as spark_sum,\n",
    "                                   min as spark_min, max as spark_max, avg as spark_avg, window, when)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self, spark, config=None):\n",
    "        \"\"\"\n",
    "        Initializes DataTransformer with configurable parameters.\n",
    "          - zscore_threshold (default: 2.0)\n",
    "          - contamination_rate for Isolation Forest (default: 0.15)\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.logger = LoggerUtility.setup_logging()\n",
    "        self.config = config or {}\n",
    "        self.zscore_threshold = self.config.get(\"zscore_threshold\", 2.0)\n",
    "        self.contamination_rate = self.config.get(\"contamination_rate\", 0.15)\n",
    "        self.logger.info(\"DataTransformer initialized.\")\n",
    "\n",
    "    def compute_expected_power(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Computes expected power output using a simplified wind power equation:\n",
    "          P = 0.5 * ρ * A * V³ * Cp\n",
    "        (with ρ=1.2, A=5024, Cp=0.45), converting from watts to MW.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Computing expected power output...\")\n",
    "            df = df.withColumn(\n",
    "                \"expected_power\",\n",
    "                0.5 * lit(1.2) *\n",
    "                lit(5024) *\n",
    "                pow(col(\"wind_speed\"), 3) *\n",
    "                lit(0.45) / lit(1_000_000)\n",
    "            )\n",
    "            self.logger.info(\"Expected power computation complete.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error computing expected power: {e}\")\n",
    "            raise\n",
    "\n",
    "    def detect_zscore_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Detects anomalies using a z‑score method.\n",
    "        Flags records as \"z_anomaly\" if the error (power_output - expected_power)\n",
    "        deviates beyond zscore_threshold * standard deviation.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Detecting anomalies using z‑score method...\")\n",
    "            df = df.withColumn(\"error\", col(\"power_output\") - col(\"expected_power\"))\n",
    "            stats = (df.select(\n",
    "                        mean(col(\"error\")).alias(\"mean_error\"),\n",
    "                        stddev(col(\"error\")).alias(\"std_error\")\n",
    "                     ).collect()[0])\n",
    "            mean_err = stats[\"mean_error\"]\n",
    "            std_err = stats[\"std_error\"]\n",
    "            self.logger.info(f\"Mean error: {mean_err}, Std error: {std_err}\")\n",
    "            df = df.withColumn(\n",
    "                \"z_anomaly\",\n",
    "                when(\n",
    "                    spark_abs(col(\"error\") - lit(mean_err)) > self.zscore_threshold * lit(std_err),\n",
    "                    lit(1)\n",
    "                ).otherwise(lit(0))\n",
    "            )\n",
    "            self.logger.info(\"Z‑score anomaly detection complete.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in z‑score anomaly detection: {e}\")\n",
    "            raise\n",
    "\n",
    "    def detect_record_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Detects record-level anomalies using Isolation Forest on features:\n",
    "        wind_speed, expected_power, and power_output.\n",
    "        Returns a new column \"if_anomaly\" (1 for anomaly, 0 otherwise).\n",
    "        Note: Converts to Pandas, so best for smaller datasets.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Detecting record-level anomalies with Isolation Forest...\")\n",
    "            df = df.withColumn(\"record_id\", monotonically_increasing_id())\n",
    "            pdf = df.select(\"record_id\", \"wind_speed\", \"expected_power\", \"power_output\").toPandas()\n",
    "            iso_forest = IsolationForest(\n",
    "                contamination=self.contamination_rate,\n",
    "                n_estimators=200,\n",
    "                max_samples='auto',\n",
    "                random_state=42\n",
    "            )\n",
    "            pdf[\"iso_score\"] = iso_forest.fit_predict(pdf[[\"wind_speed\", \"expected_power\", \"power_output\"]])\n",
    "            pdf[\"iso_score\"] = pdf[\"iso_score\"].apply(lambda x: 1 if x == -1 else 0)\n",
    "            anomaly_pdf = pdf[[\"record_id\", \"iso_score\"]]\n",
    "            anomaly_sdf = self.spark.createDataFrame(anomaly_pdf)\n",
    "            df = df.join(anomaly_sdf, on=\"record_id\", how=\"left\")\n",
    "            df = (df\n",
    "                  .withColumnRenamed(\"iso_score\", \"if_anomaly\")\n",
    "                  .withColumn(\"power_output\", round(\"power_output\", 2))\n",
    "                  .withColumn(\"expected_power\", round(col(\"expected_power\"), 2))\n",
    "                  .withColumn(\"error\", round(col(\"error\"), 2))\n",
    "                  .drop(\"record_id\")\n",
    "                  )\n",
    "            self.logger.info(\"Isolation Forest anomaly detection complete.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in record anomaly detection: {e}\")\n",
    "            raise\n",
    "\n",
    "    def combine_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Combines z‑score and Isolation Forest anomaly flags using OR logic,\n",
    "        creating a new column \"combined_anomaly\".\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Combining anomalies using OR logic.\")\n",
    "            df = df.withColumn(\n",
    "                \"combined_anomaly\",\n",
    "                when((col(\"z_anomaly\") == 1) | (col(\"if_anomaly\") == 1), lit(1)).otherwise(lit(0))\n",
    "            )\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error combining anomalies: {e}\")\n",
    "            raise\n",
    "\n",
    "    def detect_turbine_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Identifies turbines with a high anomaly rate.\n",
    "        Classifies turbine status as:\n",
    "          - > 60% anomalies: FAULTY_SENSOR\n",
    "          - 30-60% anomalies: REVIEW_REQUIRED\n",
    "          - < 30% anomalies: NORMAL\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Detecting turbine anomalies...\")\n",
    "            turbine_anomaly_df = (df.groupBy(\"turbine_id\")\n",
    "                                  .agg(\n",
    "                                      spark_sum(\"combined_anomaly\").alias(\"total_anomalies\"),\n",
    "                                      count(\"*\").alias(\"total_records\")\n",
    "                                  )\n",
    "                                  .withColumn(\"anomaly_rate\", col(\"total_anomalies\") / col(\"total_records\")))\n",
    "            turbine_anomaly_df = turbine_anomaly_df.withColumn(\n",
    "                \"turbine_status\",\n",
    "                when(col(\"anomaly_rate\") > 0.6, \"FAULTY_SENSOR\")\n",
    "                .when(col(\"anomaly_rate\") > 0.3, \"REVIEW_REQUIRED\")\n",
    "                .otherwise(\"NORMAL\")\n",
    "            )\n",
    "            self.logger.info(\"Turbine anomaly detection complete.\")\n",
    "            return turbine_anomaly_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error detecting turbine anomalies: {e}\")\n",
    "            raise\n",
    "\n",
    "    def apply_smart_filtering(self, df: DataFrame, turbine_anomaly_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Applies filtering based on turbine status:\n",
    "          - Removes FAULTY_SENSOR turbines\n",
    "          - Leaves REVIEW_REQUIRED (flagged) and NORMAL turbines.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Applying smart filtering based on turbine status.\")\n",
    "            df = df.join(turbine_anomaly_df.select(\"turbine_id\", \"turbine_status\"),\n",
    "                         on=\"turbine_id\", how=\"left\")\n",
    "            df_filtered = df.filter(col(\"turbine_status\") != \"FAULTY_SENSOR\")\n",
    "            self.logger.info(\"Smart filtering complete.\")\n",
    "            return df_filtered\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in smart filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "    def calculate_summary_statistics(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Computes summary statistics (min, max, avg power output) per turbine over a 24‑hour window.\n",
    "        Returns turbine_id, window_start, min_power, max_power, and avg_power.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Calculating summary statistics over a 24‑hour period...\")\n",
    "            summary_df = (df.groupBy(\"turbine_id\", window(col(\"timestamp\"), \"24 hours\"))\n",
    "                          .agg(\n",
    "                              round(spark_min(\"power_output\"), 2).alias(\"min_power\"),\n",
    "                              round(spark_max(\"power_output\"), 2).alias(\"max_power\"),\n",
    "                              round(spark_avg(\"power_output\"), 2).alias(\"avg_power\")\n",
    "                          )\n",
    "                          .select(\n",
    "                              col(\"turbine_id\"),\n",
    "                              col(\"window.start\").alias(\"window_start\"),\n",
    "                              col(\"min_power\"),\n",
    "                              col(\"max_power\"),\n",
    "                              col(\"avg_power\")\n",
    "                          ))\n",
    "            self.logger.info(\"Summary statistics calculation complete.\")\n",
    "            return summary_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error calculating summary statistics: {e}\")\n",
    "            raise\n",
    "\n",
    "    def save_turbine_analysis(self, df: DataFrame, table_name: str = \"gold_turbine_analysis\", mode: str = \"overwrite\"):\n",
    "        \"\"\"\n",
    "        Saves the final turbine dataset to a Gold Delta table.\n",
    "        The write mode (overwrite/append) is configurable.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Saving results to gold_data.{table_name} with mode {mode}\")\n",
    "            self.spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold_data\")\n",
    "            df.write.mode(mode).format(\"delta\").saveAsTable(f\"gold_data.{table_name}\")\n",
    "            self.logger.info(f\"Successfully saved to gold_data.{table_name}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving turbine analysis: {e}\")\n",
    "            raise\n",
    "\n",
    "    def save_summary_table(self, df: DataFrame, table_name: str = \"gold_turbine_summary\", mode: str = \"overwrite\"):\n",
    "        \"\"\"\n",
    "        Saves summary statistics to a Gold Delta table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Saving summary statistics to gold_data.{table_name} with mode {mode}\")\n",
    "            self.spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold_data\")\n",
    "            df.write.mode(mode).format(\"delta\").saveAsTable(f\"gold_data.{table_name}\")\n",
    "            self.logger.info(f\"Successfully saved summary statistics to gold_data.{table_name}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving summary table: {e}\")\n",
    "            raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wt_transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
