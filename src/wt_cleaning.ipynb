{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b3eea8-9fdc-4dfe-a709-a7b5232b7dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/ovidiumtoma@gmail.com/wind_turbine_project/src/wt_logger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa93e376-ebac-49c6-a903-417aa6f95ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, to_timestamp, when\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self, spark, config=None):\n",
    "        self.spark = spark\n",
    "        self.logger = LoggerUtility.setup_logging()\n",
    "        self.config = config or {}\n",
    "        self.logger.info(\"DataCleaner initialized.\")\n",
    "\n",
    "    def clean_turbine_data(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Cleans the turbine dataset by:\n",
    "          - Replacing negative power_output values with nulls.\n",
    "          - Dropping rows with null values in essential columns (timestamp, power_output, wind_speed).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Cleaning turbine data...\")\n",
    "            initial_count = df.count()\n",
    "            # Replace negative power outputs with nulls.\n",
    "            df = df.withColumn(\"power_output\", when(col(\"power_output\") < 0, None).otherwise(col(\"power_output\")))\n",
    "            # Drop rows with missing values in essential columns only.\n",
    "            df_clean = df.dropna(subset=[\"timestamp\", \"power_output\", \"wind_speed\"])\n",
    "            final_count = df_clean.count()\n",
    "            self.logger.info(f\"Cleaned dataset: {initial_count} records reduced to {final_count} records after cleaning.\")\n",
    "            return df_clean\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error cleaning turbine data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def transform_turbine_data(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transforms the new turbine dataset by:\n",
    "          - Scaling power from normalized (0-1) to MW (0-4.5)\n",
    "          - Renaming columns to match the original dataset\n",
    "          - Dropping unnecessary columns.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Transforming turbine data before merging.\")\n",
    "            df_transformed = (df\n",
    "                              .withColumn(\"power_output\", col(\"Power\") * 4.5)\n",
    "                              .withColumn(\"timestamp\", to_timestamp(col(\"Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                              .withColumnRenamed(\"windspeed_100m\", \"wind_speed\")\n",
    "                              .withColumnRenamed(\"winddirection_100m\", \"wind_direction\")\n",
    "                              .drop(\"Time\", \"temperature_2m\", \"relativehumidity_2m\", \"dewpoint_2m\",\n",
    "                                    \"winddirection_10m\", \"windspeed_10m\", \"windgusts_10m\", \"Power\", \"_metadata.file_path\")\n",
    "                              .cache())\n",
    "            self.logger.info(\"Turbine data transformation complete.\")\n",
    "            return df_transformed\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error transforming turbine data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def merge_bronze_data(self, df_original: DataFrame, df_new: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Merges the original and new turbine datasets into a unified Silver dataset.\n",
    "        Applies cleaning after merging.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            original_count = df_original.count()\n",
    "            new_count = df_new.count()\n",
    "            self.logger.info(f\"Merging {original_count} original records with {new_count} new records.\")\n",
    "            merged_df = df_original.unionByName(df_new, allowMissingColumns=False).cache()\n",
    "            merged_count = merged_df.count()\n",
    "            self.logger.info(f\"Total records after merging: {merged_count}\")\n",
    "            merged_clean_df = self.clean_turbine_data(merged_df)\n",
    "            return merged_clean_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error merging turbine data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def save_silver_table(self, df: DataFrame, table_name: str):\n",
    "        \"\"\"Saves the cleaned dataset to a Silver Delta table.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Saving cleaned data to silver_data.{table_name}\")\n",
    "            self.spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver_data\")\n",
    "            df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"silver_data.{table_name}\")\n",
    "            self.logger.info(f\"Successfully saved cleaned data to silver_data.{table_name}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving silver data: {e}\")\n",
    "            raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wt_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
