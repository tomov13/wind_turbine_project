{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b3eea8-9fdc-4dfe-a709-a7b5232b7dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/ovidiumtoma@gmail.com/wind_turbine_project/src/wt_logger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa93e376-ebac-49c6-a903-417aa6f95ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, to_timestamp, when\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.logger = LoggerUtility.setup_logging()\n",
    "        self.logger.info(\"DataCleaner initialized.\")\n",
    "\n",
    "    def clean_turbine_data(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\" \n",
    "        Cleans turbine dataset:\n",
    "        - Drops rows with missing essential values\n",
    "        - Replaces negative power outputs with nulls (to be removed)\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Cleaning turbine data...\")\n",
    "\n",
    "        df = (df\n",
    "              .dropna(subset=[\"timestamp\", \"power_output\", \"wind_speed\"])\n",
    "              .withColumn(\"power_output\", when(col(\"power_output\") < 0, None).otherwise(col(\"power_output\")))\n",
    "              .dropna()\n",
    "              )\n",
    "\n",
    "        self.logger.info(f\"Cleaned dataset contains {df.count()} records after removing invalid data.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform_turbine_data(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\" \n",
    "        Transforms new turbine dataset:\n",
    "        - Scales power from normalized (0-1) to MW (0-4.5)\n",
    "        - Renames columns to match original dataset\n",
    "        - Drops unnecessary columns\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Transforming turbine data before merging.\")\n",
    "\n",
    "        df = (df\n",
    "              .withColumn(\"power_output\", col(\"Power\") * 4.5)\n",
    "              .withColumn(\"timestamp\", to_timestamp(col(\"Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "              .withColumnRenamed(\"windspeed_100m\", \"wind_speed\")\n",
    "              .withColumnRenamed(\"winddirection_100m\", \"wind_direction\")\n",
    "              .drop(\"Time\", \"temperature_2m\", \"relativehumidity_2m\", \"dewpoint_2m\",\n",
    "                    \"winddirection_10m\", \"windspeed_10m\", \"windgusts_10m\", \"Power\")\n",
    "              )\n",
    "\n",
    "        self.logger.info(\"Turbine data transformation complete.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def merge_bronze_data(self, df_original_turbine_data: DataFrame, df_new_turbine_data: DataFrame) -> DataFrame:\n",
    "        \"\"\" Merges original and new turbine datasets into a unified Silver dataset. \"\"\"\n",
    "\n",
    "        self.logger.info(f\"Merging {df_original_turbine_data.count()} original turbine records with {df_new_turbine_data.count()} new turbine records.\")\n",
    "\n",
    "        # Merge datasets\n",
    "        merged_df = df_original_turbine_data.unionByName(df_new_turbine_data, allowMissingColumns=False)\n",
    "\n",
    "        self.logger.info(f\"Total records after merging: {merged_df.count()}\")\n",
    "\n",
    "        # Apply cleaning after merging\n",
    "        merged_df = self.clean_turbine_data(merged_df)\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    def save_silver_table(self, df: DataFrame, table_name: str):\n",
    "        \"\"\" Saves the merged dataset to the Silver layer. \"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Saving cleaned data to silver_data.{table_name}\")\n",
    "        self.spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver_data\")\n",
    "        df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"silver_data.{table_name}\")\n",
    "        self.logger.info(f\"Successfully saved cleaned data to silver_data.{table_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wt_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
